import os
import json
import jsonlines
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# --------------------------
# 1. é…ç½®å‚æ•°ï¼ˆå…³é”®ï¼šç§»é™¤[SEP]ï¼Œä¸¥æ ¼æ§åˆ¶é•¿åº¦ï¼‰
# --------------------------
DATASET_PATH = "/home2/zzl/C-CoT/test_C-CoT/cot_generated_first100_flat_labeled.jsonl"
MODEL_NAME = "/home2/zzl/model/Llama-2-7b-chat-hf"
SAVE_PATH = "/home2/zzl/C-CoT/baseline/LLama/ccot_seq_model.pt"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 2  # ä¿æŒå°æ‰¹é‡ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
LR = 1e-5
EPOCHS = 3
MAX_LEN = 400  # å…³é”®ï¼šè®¾ä¸º400ï¼ˆè¿œå°äºLLaMA-2çš„4096ä¸Šé™ï¼Œç•™è¶³ä½™é‡ï¼‰
TEMPERATURE = 0.07
DTYPE = torch.float16


# --------------------------
# 2. æ•°æ®é›†ç±»ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç§»é™¤[SEP]ï¼Œæ·»åŠ é•¿åº¦æ ¡éªŒï¼‰
# --------------------------
class CoTDataset(Dataset):
    def __init__(self, path, tokenizer, max_len=400):
        self.question_samples = {}
        with jsonlines.open(path, 'r') as reader:
            for obj in reader:
                # æå–æ ¸å¿ƒå­—æ®µï¼ˆä¸åŸä»£ç ä¸€è‡´ï¼‰
                question_id = obj["raw_example"]["id"]
                question_text = obj["raw_example"]["question"]
                cot_text = obj["cot"]
                is_correct = obj["is_correct"]

                # è¿‡æ»¤æ— æ•ˆCoTï¼ˆå¢å¼ºè¿‡æ»¤é€»è¾‘ï¼‰
                if not cot_text.strip() or cot_text in ("'t", "''t"):
                    continue

                # æŒ‰é—®é¢˜IDåˆ†ç»„
                if question_id not in self.question_samples:
                    self.question_samples[question_id] = {
                        "question": question_text,
                        "pos_cots": [],
                        "neg_cots": []
                    }
                if is_correct == 1:
                    self.question_samples[question_id]["pos_cots"].append(cot_text)
                else:
                    self.question_samples[question_id]["neg_cots"].append(cot_text)

        # ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼ˆå…³é”®ï¼šç”¨ç©ºæ ¼åˆ†éš”é—®é¢˜å’ŒCoTï¼Œç§»é™¤[SEP]ï¼‰
        self.train_samples = []
        self.max_len = max_len
        self.tokenizer = tokenizer  # ä¿å­˜tokenizerç”¨äºé•¿åº¦æ ¡éªŒ

        for qid, data in self.question_samples.items():
            pos_cots = data["pos_cots"]
            neg_cots = data["neg_cots"]
            if len(pos_cots) == 0 or len(neg_cots) == 0:
                continue

            # å¤„ç†æ­£æ ·æœ¬
            for pos_cot in pos_cots:
                # å…³é”®ï¼šç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¸æ–°å¢ç‰¹æ®Šç¬¦å·
                full_text = f"{data['question']} {pos_cot}"
                # æå‰ç¼–ç æ ¡éªŒé•¿åº¦ï¼ˆé¿å…åç»­è®­ç»ƒæŠ¥é”™ï¼‰
                token_len = len(self.tokenizer.encode(full_text, truncation=False))
                if token_len > self.max_len:
                    print(f"âš ï¸ æ ·æœ¬{qid}ï¼ˆæ­£ï¼‰åŸé•¿åº¦{token_len}ï¼Œå°†è¢«æˆªæ–­è‡³{self.max_len}")
                self.train_samples.append({
                    "qid": qid,
                    "text": full_text,
                    "label": 1
                })

            # å¤„ç†è´Ÿæ ·æœ¬
            for neg_cot in neg_cots:
                full_text = f"{data['question']} {neg_cot}"
                token_len = len(self.tokenizer.encode(full_text, truncation=False))
                if token_len > self.max_len:
                    print(f"âš ï¸ æ ·æœ¬{qid}ï¼ˆè´Ÿï¼‰åŸé•¿åº¦{token_len}ï¼Œå°†è¢«æˆªæ–­è‡³{self.max_len}")
                self.train_samples.append({
                    "qid": qid,
                    "text": full_text,
                    "label": 0
                })

        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼š{len(self.question_samples)}ä¸ªé—®é¢˜ï¼Œ{len(self.train_samples)}ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        return len(self.train_samples)

    def __getitem__(self, idx):
        sample = self.train_samples[idx]
        # æ–‡æœ¬ç¼–ç ï¼ˆä¸¥æ ¼æˆªæ–­ï¼Œç¡®ä¿é•¿åº¦â‰¤MAX_LENï¼‰
        enc = self.tokenizer(
            sample["text"],
            truncation=True,  # å¼ºåˆ¶æˆªæ–­è¶…é•¿æ–‡æœ¬
            max_length=self.max_len,
            padding="max_length",  # ä¸è¶³è¡¥å…¨
            return_tensors="pt",
            add_special_tokens=True  # ä½¿ç”¨LLaMAåŸç”Ÿç‰¹æ®Šç¬¦å·ï¼ˆ<s>å¼€å¤´ï¼Œ</s>ç»“å°¾ï¼‰
        )
        # æ ¡éªŒç¼–ç åé•¿åº¦ï¼ˆ debug ç”¨ï¼Œå¯åˆ é™¤ï¼‰
        assert enc["input_ids"].shape[1] == self.max_len, f"ç¼–ç åé•¿åº¦å¼‚å¸¸ï¼š{enc['input_ids'].shape[1]}"

        return {
            "qid": sample["qid"],
            "input_ids": enc["input_ids"].squeeze(0),  # [max_len]
            "attention_mask": enc["attention_mask"].squeeze(0),  # [max_len]
            "label": sample["label"]
        }


# --------------------------
# 3. æ¨¡å‹å°è£…ï¼ˆæ— ä¿®æ”¹ï¼Œç¡®ä¿ä½ç½®åµŒå…¥åŒ¹é…ï¼‰
# --------------------------
class LlamaEncoder(nn.Module):
    def __init__(self, model_name, dtype=torch.float16):
        super().__init__()
        self.model = AutoModel.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
            low_cpu_mem_usage=True,
            # å…³é”®ï¼šæ˜¾å¼æŒ‡å®šæ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¸Tokenizerä¸€è‡´ï¼‰
            max_position_embeddings=4096
        )
        hidden_size = self.model.config.hidden_size
        self.proj = nn.Linear(hidden_size, hidden_size, dtype=dtype)
        # å†»ç»“ä¸»ä½“ï¼Œè®­ç»ƒæŠ•å½±å±‚
        for param in self.model.parameters():
            param.requires_grad = False
        for param in self.proj.parameters():
            param.requires_grad = True

    def forward(self, input_ids, attention_mask):
        with torch.no_grad() if not self.training else torch.enable_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
        # å–æœ€åä¸€å±‚æœ€åä¸€ä¸ªtokençš„åµŒå…¥
        last_hidden = outputs.last_hidden_state  # [B, L, H]
        seq_emb = last_hidden[:, -1, :]  # [B, H]
        seq_emb = self.proj(seq_emb)
        seq_emb = F.normalize(seq_emb, dim=-1)
        return seq_emb


# --------------------------
# 4. InfoNCEæŸå¤±ï¼ˆæ— ä¿®æ”¹ï¼‰
# --------------------------
def info_nce_loss(embeddings, labels, qids, temperature=0.07):
    B, H = embeddings.shape
    device = embeddings.device

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature
    sim_matrix = sim_matrix - torch.eye(B, device=device) * 1e12  # å±è”½è‡ªèº«

    # æ„å»ºæ­£ä¾‹æ©ç 
    pos_mask = torch.zeros((B, B), device=device)
    for i in range(B):
        if labels[i] == 1:
            same_qid = (qids == qids[i])
            is_pos = (labels == 1)
            pos_mask[i] = (same_qid & is_pos).float()

    # æ„å»ºè´Ÿä¾‹æ©ç 
    neg_mask = 1 - pos_mask - torch.eye(B, device=device)
    neg_mask = neg_mask.clamp(0, 1)

    # è®¡ç®—æŸå¤±
    pos_score = (sim_matrix * pos_mask).sum(dim=1, keepdim=True)
    neg_score = (sim_matrix * neg_mask).view(B, -1)
    logits = torch.cat([pos_score, neg_score], dim=1)
    target = torch.zeros(B, dtype=torch.long, device=device)
    loss = F.cross_entropy(logits, target)

    return loss


# --------------------------
# 5. è®­ç»ƒå¾ªç¯ï¼ˆæ·»åŠ CUDAè°ƒè¯•å¼€å…³ï¼‰
# --------------------------
def train():
    # æ­¥éª¤1ï¼šåŠ è½½Tokenizerï¼ˆå…³é”®ï¼šä¸æ–°å¢[SEP]ï¼Œç”¨åŸç”Ÿç¬¦å·ï¼‰
    print(">>> åŠ è½½Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # LLaMAåŸç”Ÿeos_tokenä½œä¸ºpad_token
    # æ‰“å°Tokenizerä¿¡æ¯ï¼ˆ debug ç”¨ï¼‰
    print(f"Tokenizerä¿¡æ¯ï¼špad_token={tokenizer.pad_token}, eos_token={tokenizer.eos_token}")
    print(f"æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦ï¼š{tokenizer.model_max_length}")

    # æ­¥éª¤2ï¼šåŠ è½½æ•°æ®é›†
    print(">>> åŠ è½½æ•°æ®é›†...")
    dataset = CoTDataset(DATASET_PATH, tokenizer, max_len=MAX_LEN)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    # æ­¥éª¤3ï¼šåŠ è½½æ¨¡å‹
    print(">>> åŠ è½½LLaMA-2æ¨¡å‹...")
    model = LlamaEncoder(MODEL_NAME, dtype=DTYPE).to(DEVICE)
    if os.path.exists(SAVE_PATH):
        model.proj.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
        print(f"âœ… åŠ è½½å·²æœ‰æƒé‡ï¼š{SAVE_PATH}")

    # æ­¥éª¤4ï¼šåˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=LR,
        weight_decay=1e-4
    )
    grad_clip = torch.nn.utils.clip_grad_norm_
    max_grad_norm = 1.0

    # æ­¥éª¤5ï¼šè®­ç»ƒï¼ˆå…³é”®ï¼šæ·»åŠ CUDA_BLOCKINGï¼Œæ–¹ä¾¿è°ƒè¯•ï¼‰
    print(">>> å¼€å§‹è®­ç»ƒ...")
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0.0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS} | Loss: ---")
        for batch in pbar:
            # è¯»å–æ•°æ®ï¼ˆæ·»åŠ blocking=Trueï¼Œç¡®ä¿CUDAé”™è¯¯å®šä½ï¼‰
            input_ids = batch["input_ids"].to(DEVICE, dtype=torch.long, non_blocking=False)
            attention_mask = batch["attention_mask"].to(DEVICE, dtype=torch.long, non_blocking=False)
            labels = batch["label"].to(DEVICE, dtype=torch.long, non_blocking=False)
            qids = batch["qid"]

            # å‰å‘ä¼ æ’­ï¼ˆæ•è·å¼‚å¸¸ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰
            try:
                embeddings = model(input_ids, attention_mask)
            except Exception as e:
                print(f"\nâŒ å‰å‘ä¼ æ’­é”™è¯¯ï¼šinput_idså½¢çŠ¶={input_ids.shape}, attention_maskå½¢çŠ¶={attention_mask.shape}")
                print(f"input_idsæ ·æœ¬ï¼š{input_ids[0][:10]}...")  # æ‰“å°å‰10ä¸ªTokenï¼Œçœ‹æ˜¯å¦å¼‚å¸¸
                raise e

            # è®¡ç®—æŸå¤±
            loss = info_nce_loss(embeddings, labels, qids, temperature=TEMPERATURE)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            grad_clip(model.parameters(), max_grad_norm)
            optimizer.step()

            # ç´¯è®¡æŸå¤±
            total_loss += loss.item() * input_ids.size(0)
            pbar.set_description(f"Epoch {epoch+1}/{EPOCHS} | Loss: {loss.item():.4f}")

        # ä¿å­˜æ¨¡å‹
        epoch_avg_loss = total_loss / len(dataset)
        print(f"ğŸ“Š Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±ï¼š{epoch_avg_loss:.4f}")
        torch.save(model.proj.state_dict(), SAVE_PATH)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³ï¼š{SAVE_PATH}\n")

    print(">>> è®­ç»ƒç»“æŸï¼")


# --------------------------
# 6. ä¸»å‡½æ•°ï¼ˆæ·»åŠ CUDAè°ƒè¯•ç¯å¢ƒå˜é‡ï¼‰
# --------------------------
if __name__ == "__main__":
    # å›ºå®šéšæœºç§å­
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        # å…³é”®ï¼šå¯ç”¨CUDAåŒæ­¥ï¼Œç¡®ä¿é”™è¯¯å®šä½å‡†ç¡®ï¼ˆè®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ï¼Œè°ƒè¯•å®Œæˆåå¯æ³¨é‡Šï¼‰
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
        os.environ["TORCH_USE_CUDA_DSA"] = "1"  # å¯ç”¨è®¾å¤‡ç«¯æ–­è¨€ï¼Œæ˜¾ç¤ºè¯¦ç»†é”™è¯¯
    # å¯åŠ¨è®­ç»ƒ
    train()