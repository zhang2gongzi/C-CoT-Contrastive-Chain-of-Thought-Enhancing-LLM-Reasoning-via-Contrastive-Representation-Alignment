import json
import re
import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# ===== é…ç½® =====
MODEL_PATH = "/home2/zzl/model_eval/modelscope_models/Qwen/Qwen-7B-Chat"
VAL_PATH = "/home2/zzl/ChatLogic/PARARULE-Plus/Depth2/PARARULE_Plus_Depth2_shuffled_dev_huggingface.jsonl"
SAVE_PATH = "cot_generated_first10_flat_labeled.jsonl"

NUM_EXAMPLES = 10       # è¯»å–å‰ 10 æ¡
N_SAMPLES = 4           # æ¯é¢˜ç”Ÿæˆ N æ¡ CoT
MAX_NEW_TOKENS = 256
TEMPERATURE = 0.7
TOP_P = 0.9
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ===== åŠ è½½æ¨¡å‹ =====
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True).eval()

# ===== è¯»å–å‰ NUM_EXAMPLES æ¡éªŒè¯é›† =====
dataset = []
with open(VAL_PATH, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= NUM_EXAMPLES:
            break
        dataset.append(json.loads(line))

# ===== ä» CoT æ–‡æœ¬è§£ææ¨¡å‹çš„æœ€ç»ˆå›ç­”ï¼ˆYes/No â†’ labelï¼‰ =====
def extract_label_from_cot(cot_text):
    text = cot_text.strip().lower()

    match = re.search(r"answer[:ï¼š]?\s*(yes|no)", text)
    if match:
        return 1 if match.group(1) == "yes" else 0

    if "yes" in text and "no" not in text:
        return 1
    elif "no" in text and "yes" not in text:
        return 0

    return None

# ===== åˆ¤æ–­æ˜¯å¦æ­£ç¡® =====
def judge_correctness(pred_label, gold_label):
    if pred_label is None:
        return 0
    return 1 if pred_label == gold_label else 0

# ===== æ¨ç†å¹¶å­˜å‚¨ =====
total_correct = 0
total_count = 0

with open(SAVE_PATH, "w", encoding="utf-8") as fout:
    for example in tqdm(dataset, desc="Generating CoTs"):
        prompt = f"Q: {example['question']}\nLet's think step by step, and then answer yes or no."
        gold_label = example["label"]

        for _ in range(N_SAMPLES):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                temperature=TEMPERATURE,
                top_p=TOP_P
            )
            cot_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            cot_text = cot_text[len(prompt):].strip()

            pred_label = extract_label_from_cot(cot_text)
            correctness = judge_correctness(pred_label, gold_label)

            fout.write(json.dumps({
                "idx": example.get("idx"),
                "raw_example": example,
                "prompt": prompt,
                "cot": cot_text,
                "pred_label": pred_label,
                "gold_label": gold_label,
                "is_correct": correctness
            }, ensure_ascii=False) + "\n")

            total_count += 1
            total_correct += correctness

# ===== è¾“å‡ºå‡†ç¡®ç‡ =====
accuracy = total_correct / total_count if total_count > 0 else 0
print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜åˆ° {SAVE_PATH}")
print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_count}, æ­£ç¡®æ•°: {total_correct}, å‡†ç¡®ç‡: {accuracy:.2%}")
